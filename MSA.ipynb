{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZYS1F_en0727"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import script as util\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CEZds5Vv2BXO"
      },
      "outputs": [],
      "source": [
        "## Path to Dataset\n",
        "Data_Path = '/content/aligned_50.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ArLZpLzg5k3T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading pickle file: [Errno 2] No such file or directory: '/content/aligned_50.pkl'\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  with open(Data_Path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "except Exception as e:\n",
        "  print(f\"Error loading pickle file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nBKywcYH1R3u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-Level Keys in Dataset: ['train', 'valid', 'test']\n",
            "\n",
            "Field: train\n",
            "Number of sub-fields in Field: 9\n",
            "Example sub-fields: ['raw_text', 'audio', 'vision', 'id', 'text', 'text_bert', 'annotations', 'classification_labels', 'regression_labels']\n",
            "\n",
            "\tSub-field: raw_text\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (1284,)\n",
            "\n",
            "\tSub-field: audio\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (1284, 50, 5)\n",
            "\n",
            "\tSub-field: vision\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (1284, 50, 20)\n",
            "\n",
            "\tSub-field: id\n",
            "\tType of data: <class 'list'>\n",
            "\tShape of Data: 1284\n",
            "\n",
            "\tSub-field: text\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (1284, 50, 768)\n",
            "\n",
            "\tSub-field: text_bert\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (1284, 3, 50)\n",
            "\n",
            "\tSub-field: annotations\n",
            "\tType of data: <class 'list'>\n",
            "\tShape of Data: 1284\n",
            "\n",
            "\tSub-field: classification_labels\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (1284,)\n",
            "\n",
            "\tSub-field: regression_labels\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (1284,)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Field: valid\n",
            "Number of sub-fields in Field: 9\n",
            "Example sub-fields: ['raw_text', 'audio', 'vision', 'id', 'text', 'text_bert', 'annotations', 'classification_labels', 'regression_labels']\n",
            "\n",
            "\tSub-field: raw_text\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (229,)\n",
            "\n",
            "\tSub-field: audio\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (229, 50, 5)\n",
            "\n",
            "\tSub-field: vision\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (229, 50, 20)\n",
            "\n",
            "\tSub-field: id\n",
            "\tType of data: <class 'list'>\n",
            "\tShape of Data: 229\n",
            "\n",
            "\tSub-field: text\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (229, 50, 768)\n",
            "\n",
            "\tSub-field: text_bert\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (229, 3, 50)\n",
            "\n",
            "\tSub-field: annotations\n",
            "\tType of data: <class 'list'>\n",
            "\tShape of Data: 229\n",
            "\n",
            "\tSub-field: classification_labels\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (229,)\n",
            "\n",
            "\tSub-field: regression_labels\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (229,)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Field: test\n",
            "Number of sub-fields in Field: 9\n",
            "Example sub-fields: ['raw_text', 'audio', 'vision', 'id', 'text', 'text_bert', 'annotations', 'classification_labels', 'regression_labels']\n",
            "\n",
            "\tSub-field: raw_text\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (686,)\n",
            "\n",
            "\tSub-field: audio\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (686, 50, 5)\n",
            "\n",
            "\tSub-field: vision\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (686, 50, 20)\n",
            "\n",
            "\tSub-field: id\n",
            "\tType of data: <class 'list'>\n",
            "\tShape of Data: 686\n",
            "\n",
            "\tSub-field: text\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (686, 50, 768)\n",
            "\n",
            "\tSub-field: text_bert\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (686, 3, 50)\n",
            "\n",
            "\tSub-field: annotations\n",
            "\tType of data: <class 'list'>\n",
            "\tShape of Data: 686\n",
            "\n",
            "\tSub-field: classification_labels\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (686,)\n",
            "\n",
            "\tSub-field: regression_labels\n",
            "\tType of data: <class 'numpy.ndarray'>\n",
            "\tShape of Data: (686,)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    Dataset_dict = util.inspect_dataset(data)\n",
        "except SystemExit as e:\n",
        "    print(f\"SystemExit with code: {e.code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvzhPkpi7UJs",
        "outputId": "e72a44b7-c1c7-4cf6-a330-3735412d5ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset_dict:\n",
            "\tKeys: dict_keys(['train', 'valid', 'test'])\n",
            "\t\tSub-keys in Train: dict_keys(['audio', 'vision', 'id', 'text', 'classification_labels', 'regression_labels'])\n",
            "\t\t\tLength of Train id: 1284\n",
            "\t\t\tShape of Train audio: (1284, 50, 5)\n",
            "\t\t\tShape of Train vision: (1284, 50, 20)\n",
            "\t\t\tShape of Train text: (1284, 50, 768)\n",
            "\t\t\tShape of Train classification_labels: (1284,)\n",
            "\t\t\tShape of Train regression_labels: (1284,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset_dict:\")\n",
        "\n",
        "print(f\"\\tKeys: {Dataset_dict.keys()}\")\n",
        "print(f\"\\t\\tSub-keys in Train: {Dataset_dict['train'].keys()}\")\n",
        "\n",
        "print(f\"\\t\\t\\tLength of Train id: {len(Dataset_dict['train']['id'])}\")\n",
        "print(f\"\\t\\t\\tShape of Train audio: {Dataset_dict['train']['audio'].shape}\")\n",
        "print(f\"\\t\\t\\tShape of Train vision: {Dataset_dict['train']['vision'].shape}\")\n",
        "print(f\"\\t\\t\\tShape of Train text: {Dataset_dict['train']['text'].shape}\")\n",
        "print(f\"\\t\\t\\tShape of Train classification_labels: {Dataset_dict['train']['classification_labels'].shape}\")\n",
        "print(f\"\\t\\t\\tShape of Train regression_labels: {Dataset_dict['train']['regression_labels'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_3aa2Roi-ThR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b3bmdCe8yOw",
        "outputId": "624da956-3f53-4190-d5ca-8961fccbdbfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device currently in Use:cpu\n"
          ]
        }
      ],
      "source": [
        "# Initialize GPU device (if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device currently in Use:{device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "x_RJmRhR-bAp"
      },
      "outputs": [],
      "source": [
        "## These Constants will be used later on various steps\n",
        "## I am defining all of them here\n",
        "## These are mainly parameters and hyperparameters\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "DROP = 0.4\n",
        "HID = 256\n",
        "P_HID = 64\n",
        "\n",
        "MAX_EPOCH = 20\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "VISUAL_DIM = 20\n",
        "ACOUSTIC_DIM = 5\n",
        "LANGUAGE_DIM = 768\n",
        "OUTPUT_DIM = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NnuAcbeMMNHC"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, dataset: dict):\n",
        "        \"\"\"\n",
        "        Initializes the dataset with splits: train, valid, and test.\n",
        "        Expects dataset to be a dict with keys: 'train', 'valid', 'test'.\n",
        "        Each split should have sub-keys 'audio', 'vision', 'text',\n",
        "        'classification_labels', and optionally 'regression_labels'.\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "\n",
        "        self.train_data = []\n",
        "        self.valid_data = []\n",
        "        self.test_data = []\n",
        "\n",
        "        self._process_data()\n",
        "\n",
        "    def _process_data(self):\n",
        "        \"\"\"\n",
        "        Process and convert each sample in the dataset to tensors.\n",
        "        \"\"\"\n",
        "        for split in ['train', 'valid', 'test']:\n",
        "            if split not in self.dataset:\n",
        "                raise ValueError(f\"Expected split '{split}' not found in dataset keys {list(self.dataset.keys())}\")\n",
        "\n",
        "            split_data = self.dataset[split]\n",
        "            split_list = []  # temporary list to hold processed samples for current split\n",
        "\n",
        "            for i in range(len(split_data['id'])):\n",
        "                sample = {\n",
        "                    'audio': torch.tensor(split_data['audio'][i], dtype=torch.float32),\n",
        "                    'vision': torch.tensor(split_data['vision'][i], dtype=torch.float32),\n",
        "                    'text': torch.tensor(split_data['text'][i], dtype=torch.float32),\n",
        "                    'labels': torch.tensor(split_data['classification_labels'][i], dtype=torch.long),\n",
        "                }\n",
        "\n",
        "                split_list.append(sample)\n",
        "\n",
        "            if split == 'train':\n",
        "                self.train_data = split_list\n",
        "            elif split == 'valid':\n",
        "                self.valid_data = split_list\n",
        "            else:\n",
        "                self.test_data = split_list\n",
        "\n",
        "            print(f\"Processed {len(split_list)} samples for split '{split}'\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the total number of samples across all splits.\n",
        "        \"\"\"\n",
        "        return len(self.train_data) + len(self.valid_data) + len(self.test_data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        \"\"\"\n",
        "        Retrieves a sample given a global index.\n",
        "        \"\"\"\n",
        "        if idx < len(self.train_data):\n",
        "            return self.train_data[idx]\n",
        "        elif idx < len(self.train_data) + len(self.valid_data):\n",
        "            return self.valid_data[idx - len(self.train_data)]\n",
        "        else:\n",
        "            return self.test_data[idx - len(self.train_data) - len(self.valid_data)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "oeqKut4-_SQB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_dataloader(dataset, batch_size=32):\n",
        "  \"\"\"\n",
        "  Creates DataLoader instances for train, validation, and test sets.\n",
        "\n",
        "  Args:\n",
        "      dataset: Raw dataset containing multimodal features\n",
        "      batch_size: Batch size for DataLoaders (default: 32)\n",
        "\n",
        "  Returns:\n",
        "      tuple: (train_loader, dev_loader, test_loader)\n",
        "  \"\"\"\n",
        "\n",
        "  # Create the custom dataset\n",
        "  custom_dataset = MyDataset(dataset)\n",
        "\n",
        "  # Create index ranges for each split\n",
        "  train_indices = range(len(custom_dataset.train_data))\n",
        "  dev_indices = range(\n",
        "      len(custom_dataset.train_data),\n",
        "      len(custom_dataset.train_data) + len(custom_dataset.valid_data)\n",
        "  )\n",
        "  test_indices = range(\n",
        "      len(custom_dataset.train_data) + len(custom_dataset.valid_data),\n",
        "      len(custom_dataset)\n",
        "  )\n",
        "\n",
        "  # Create subset datasets\n",
        "  train_dataset = Subset(custom_dataset, train_indices)\n",
        "  valid_dataset = Subset(custom_dataset, dev_indices)\n",
        "  test_dataset = Subset(custom_dataset, test_indices)\n",
        "\n",
        "   # Wrap the Subset instances with DataLoader for batching and shuffling\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, valid_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd04ka2ZR3Wi",
        "outputId": "104befb7-9ba7-4e55-a779-08e37b91c78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 1284 samples for split 'train'\n",
            "Processed 229 samples for split 'valid'\n",
            "Processed 686 samples for split 'test'\n"
          ]
        }
      ],
      "source": [
        "train_loader, valid_loader, test_loader = create_dataloader(Dataset_dict, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPYUCT_DSFG9",
        "outputId": "24717d1e-e65f-4bc9-a509-2836c05661d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audio Shape: torch.Size([16, 50, 5])\n",
            "Vision Shape: torch.Size([16, 50, 20])\n",
            "Text Shape: torch.Size([16, 50, 768])\n",
            "Labels Shape: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    audio = batch['audio']\n",
        "    vision = batch['vision']\n",
        "    text = batch['text']\n",
        "    labels = batch['labels']  # Assuming your dataset has 'classification_labels'\n",
        "\n",
        "    print(f\"Audio Shape: {audio.shape}\")\n",
        "    print(f\"Vision Shape: {vision.shape}\")\n",
        "    print(f\"Text Shape: {text.shape}\")\n",
        "    print(f\"Labels Shape: {labels.shape}\")\n",
        "    break  # Just print the first batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8izK2uEYTw7V"
      },
      "outputs": [],
      "source": [
        "class MSAModel(nn.Module):\n",
        "    def __init__(self, visual_dim, acoustic_dim, language_dim, hidden_dim, pred_hidden_dim, dropout_value, output_dim):\n",
        "        super(MSAModel, self).__init__()\n",
        "        self.d_v = visual_dim\n",
        "        self.d_a = acoustic_dim\n",
        "        self.d_l = language_dim\n",
        "        self.Hid = hidden_dim\n",
        "        self.P_h = pred_hidden_dim\n",
        "        self.Drop = dropout_value\n",
        "        self.Output = output_dim\n",
        "\n",
        "        self.mlp_a = self.sLSTM_MLP(self.d_a, self.Hid)\n",
        "        self.mlp_v = self.sLSTM_MLP(self.d_v, self.Hid)\n",
        "        self.mlp_t = self.BERT_MLP(self.d_l, self.Hid)\n",
        "\n",
        "        self.E_m_p_a = self.Private_Encoder(self.Hid, self.Hid)\n",
        "        self.E_m_c_a = self.Shared_Encoder(self.Hid, self.Hid)\n",
        "        self.D_m_a = self.Decoder(self.Hid, self.Hid)\n",
        "\n",
        "        self.E_m_p_v = self.Private_Encoder(self.Hid, self.Hid)\n",
        "        self.E_m_c_v = self.Shared_Encoder(self.Hid, self.Hid)\n",
        "        self.D_m_v = self.Decoder(self.Hid, self.Hid)\n",
        "\n",
        "        self.E_m_p_t = self.Private_Encoder(self.Hid, self.Hid)\n",
        "        self.E_m_c_t = self.Shared_Encoder(self.Hid, self.Hid)\n",
        "        self.D_m_t = self.Decoder(self.Hid, self.Hid)\n",
        "\n",
        "        self.M_m_a = self.MAN(self.Hid)\n",
        "        self.M_m_v = self.MAN(self.Hid)\n",
        "        self.M_m_t = self.MAN(self.Hid)\n",
        "\n",
        "        self.M_al = self.MLF(self.Hid)\n",
        "        self.M_av = self.MLF(self.Hid)\n",
        "        self.M_lv = self.MLF(self.Hid)\n",
        "\n",
        "        self.M_alav = self.MLF(self.Hid)\n",
        "        self.M_alv = self.MLF(self.Hid)\n",
        "        self.M_allv = self.MLF(self.Hid)\n",
        "        self.M_avl = self.MLF(self.Hid)\n",
        "        self.M_lva = self.MLF(self.Hid)\n",
        "        self.M_avlv = self.MLF(self.Hid)\n",
        "\n",
        "\n",
        "        self.P = self.Prediction(self.Hid, self.P_h, self.Output, self.Drop)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.01)\n",
        "            elif isinstance(m, nn.LSTM):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight' in name:\n",
        "                        nn.init.xavier_uniform_(param)\n",
        "                    elif 'bias' in name:\n",
        "                        param.data.fill_(0)\n",
        "\n",
        "    def Private_Encoder(self, input_dim, hidden_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def Shared_Encoder(self, input_dim, hidden_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def Decoder(self, input_dim, hidden_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def sLSTM_MLP(self, input_dim, hidden_dim):\n",
        "        class sLSTMBlock(nn.Module):\n",
        "            def __init__(self, input_dim, hidden_dim):\n",
        "                super(sLSTMBlock, self).__init__()\n",
        "                self.lstm1 = nn.LSTM(input_dim, input_dim, batch_first=True)\n",
        "                self.norm1 = nn.LayerNorm(input_dim)\n",
        "                self.lstm2 = nn.LSTM(input_dim, input_dim, batch_first=True)\n",
        "                self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "                self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "            def forward(self, x):\n",
        "                x, (h_n, _) = self.lstm1(x)\n",
        "                x = self.norm1(h_n[-1])\n",
        "                x = x.unsqueeze(1)\n",
        "                x, (h_n, _) = self.lstm2(x)\n",
        "                x = self.fc(h_n[-1])\n",
        "                x = self.norm2(x)\n",
        "                return x\n",
        "        return sLSTMBlock(input_dim, hidden_dim)\n",
        "\n",
        "    def BERT_MLP(self, input_dim, hidden_dim):\n",
        "        class BERTMLP(nn.Module):\n",
        "            def __init__(self, input_dim, hidden_dim):\n",
        "                super(BERTMLP, self).__init__()\n",
        "                self.mlp = nn.Sequential(\n",
        "                    nn.Linear(input_dim, hidden_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.LayerNorm(hidden_dim)\n",
        "                )\n",
        "                self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "            def forward(self, x):\n",
        "                token_features = self.mlp(x)\n",
        "                lstm_out, (h_n, c_n) = self.lstm(token_features)\n",
        "                aggregated_features = h_n.squeeze(0)\n",
        "                return aggregated_features\n",
        "\n",
        "        return BERTMLP(input_dim, hidden_dim)\n",
        "\n",
        "    def MAN(self, hidden_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def MLF(self, hidden_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim, 64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(64, hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def Prediction(self, hidden_dim, pred_hid_dim, output_dim, dropout_val):\n",
        "        return nn.Sequential(\n",
        "            nn.LayerNorm(3 * hidden_dim),\n",
        "            nn.Dropout(dropout_val),\n",
        "            nn.Linear(3 * hidden_dim, pred_hid_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(pred_hid_dim, output_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(output_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, data_v, data_a, words):\n",
        "        O_a = self.mlp_a(data_a)\n",
        "        O_v = self.mlp_v(data_v)\n",
        "        O_l = self.mlp_t(words)\n",
        "\n",
        "\n",
        "        h_a_c = self.E_m_c_a(O_a)\n",
        "        h_a_p = self.E_m_p_a(O_a)\n",
        "        h_a = h_a_c + h_a_p\n",
        "\n",
        "        h_v_c = self.E_m_c_v(O_v)\n",
        "        h_v_p = self.E_m_p_v(O_v)\n",
        "        h_v = h_v_c + h_v_p\n",
        "\n",
        "        h_l_c = self.E_m_c_t(O_l)\n",
        "        h_l_p = self.E_m_p_t(O_l)\n",
        "        h_l = h_l_c + h_l_p\n",
        "\n",
        "        h_a_1 = self.D_m_a(h_a)\n",
        "        h_v_1 = self.D_m_v(h_v)\n",
        "        h_l_1 = self.D_m_t(h_l)\n",
        "\n",
        "        M_a = self.M_m_a(h_a)\n",
        "        M_v = self.M_m_v(h_v)\n",
        "        M_l = self.M_m_t(h_l)\n",
        "\n",
        "        M_al = self.M_al(torch.cat([M_a, M_l], dim=-1))\n",
        "        M_av = self.M_av(torch.cat([M_a, M_v], dim=-1))\n",
        "        M_lv = self.M_lv(torch.cat([M_l, M_v], dim=-1))\n",
        "\n",
        "        M_alav = self.M_alav(torch.cat([M_al, M_av], dim=-1))\n",
        "        M_alv  = self.M_alv(torch.cat([M_al, M_v], dim=-1))\n",
        "        M_allv = self.M_allv(torch.cat([M_al, M_lv], dim=-1))\n",
        "        M_avl  = self.M_avl(torch.cat([M_av, M_l], dim=-1))\n",
        "        M_lva  = self.M_lva(torch.cat([M_lv, M_a], dim=-1))\n",
        "        M_avlv = self.M_avlv(torch.cat([M_av, M_lv], dim=-1))\n",
        "\n",
        "        M_uni = M_a + M_l + M_v\n",
        "        M_bi  = M_al + M_av + M_lv\n",
        "        M_tri = M_alav + M_alv + M_allv + M_avl + M_lva + M_avlv\n",
        "\n",
        "        Fusion = torch.cat([M_uni, M_bi, M_tri], dim=-1)\n",
        "        Pred = self.P(Fusion)\n",
        "\n",
        "        H_m = (h_a_c, h_a_p, h_v_c, h_v_p, h_l_c, h_l_p, h_a_1, h_v_1, h_l_1)\n",
        "        return H_m, Pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NepF_BvTWnCv"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, num_epochs=MAX_EPOCH, device=device):\n",
        "    \"\"\"\n",
        "    Train the model with given hyperparameters, data loaders, and optimizer.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model.\n",
        "        train_loader: DataLoader for training data.\n",
        "        val_loader: DataLoader for validation data.\n",
        "        optimizer: Optimizer for training.\n",
        "        num_epochs: Number of epochs to train (default: MAX_EPOCH).\n",
        "        device: Device to run training on (e.g., \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "        history: Dictionary with training and validation loss and accuracy over epochs.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Learning rate scheduler for stability\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "    # Gradient clipping parameter\n",
        "    max_grad_norm = 1.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        total_train_correct = 0.0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            try:\n",
        "                # Extract and send data to the device\n",
        "                acoustic = batch['audio'].to(device, dtype=torch.float32)\n",
        "                visual   = batch['vision'].to(device, dtype=torch.float32)\n",
        "                words    = batch['text'].to(device, dtype=torch.float32)\n",
        "                labels   = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass: model expects (visual, acoustic, words)\n",
        "                H_m, Y_pred = model(visual, acoustic, words)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = nn.CrossEntropyLoss()(Y_pred, labels)\n",
        "                loss.backward()\n",
        "\n",
        "                # Apply gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Compute accuracy\n",
        "                preds = torch.argmax(Y_pred, dim=1)\n",
        "                total_train_correct += (preds == labels).sum().item()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing training batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        total_val_correct = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                try:\n",
        "                    acoustic = batch['audio'].to(device, dtype=torch.float32)\n",
        "                    visual   = batch['vision'].to(device, dtype=torch.float32)\n",
        "                    words    = batch['text'].to(device, dtype=torch.float32)\n",
        "                    labels   = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "                    H_m, Y_pred = model(visual, acoustic, words)\n",
        "\n",
        "                    loss = nn.CrossEntropyLoss()(Y_pred, labels)\n",
        "                    total_val_loss += loss.item()\n",
        "\n",
        "                    preds = torch.argmax(Y_pred, dim=1)\n",
        "                    total_val_correct += (preds == labels).sum().item()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing validation batch {batch_idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Average losses and accuracies\n",
        "        avg_train_loss = total_train_loss / max(1, len(train_loader))\n",
        "        avg_val_loss   = total_val_loss / max(1, len(val_loader))\n",
        "        train_acc = 100 * total_train_correct / len(train_loader.dataset)\n",
        "        val_acc   = 100 * total_val_correct / len(val_loader.dataset)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Step scheduler based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Save model if validation loss improves\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_loss': best_val_loss\n",
        "            }, 'Best_MSA_model.pth')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total training time: {end_time - start_time:.2f} seconds\")\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3qmUdL9Cs8BC"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader, device=device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    total_test_loss = 0.0\n",
        "    total_test_correct = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_loader):\n",
        "            try:\n",
        "                acoustic = batch['audio'].to(device, dtype=torch.float32)\n",
        "                visual   = batch['vision'].to(device, dtype=torch.float32)\n",
        "                words    = batch['text'].to(device, dtype=torch.float32)\n",
        "                labels   = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "                H_m, Y_pred = model(visual, acoustic, words)\n",
        "\n",
        "                loss = nn.CrossEntropyLoss()(Y_pred, labels)\n",
        "                total_test_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(Y_pred, dim=1)\n",
        "                total_test_correct += (preds == labels).sum().item()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing test batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "    avg_test_loss = total_test_loss / max(1, len(test_loader))\n",
        "    test_acc = 100 * total_test_correct / len(test_loader.dataset)\n",
        "    print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "    return avg_test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xZdHO6QuARk"
      },
      "outputs": [],
      "source": [
        "def run(model, train_loader, val_loader, test_loader, optimizer, model_path='Best_MSA_model.pth', device='cuda'):\n",
        "    try:\n",
        "        # Try to load the saved model\n",
        "        checkpoint = torch.load(model_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(\"Loaded pre-trained model.\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        evaluate(model, test_loader, device)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # If no saved model, train the model from scratch\n",
        "        print(\"Training model from scratch.\")\n",
        "\n",
        "        history = train(model, train_loader, valid_loader, optimizer, num_epochs=20, device=device)\n",
        "\n",
        "        # Plot both loss and accuracy in one figure with two subplots\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Subplot for loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history['train_loss'], label='Train Loss')\n",
        "        plt.plot(history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Subplot for accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "        plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "        plt.title('Training and Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('loss_and_acc_history.png')\n",
        "        plt.show()\n",
        "\n",
        "        # Evaluate on test set after training\n",
        "        evaluate(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "b4v0UXv1Y_aL"
      },
      "outputs": [],
      "source": [
        "model = MSAModel(VISUAL_DIM, ACOUSTIC_DIM, LANGUAGE_DIM, HID, P_HID, DROP, OUTPUT_DIM)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "-Gq3tMVlucfb",
        "outputId": "003f9ffb-d76a-4b4f-c5c6-c4013fa97a72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7648\\2114175058.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pre-trained model.\n",
            "Average Test Loss: 0.7460\n",
            "Test Accuracy: 74.49%\n"
          ]
        }
      ],
      "source": [
        "run(model,train_loader, valid_loader, test_loader, optimizer, model_path=\"Best_MSA_model.pth\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUjAhSdNZyRi",
        "outputId": "2d690726-7072-4c20-87ec-c7277d4410e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Test Loss: 0.7460\n",
            "Test Accuracy: 74.49%\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(model, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
